import torch
from torch.utils.data import Dataset
import numpy as np
import pandas as pd
import os
from copy import deepcopy
from sklearn.model_selection import train_test_split
from config import config
from torch.nn.utils.rnn import pad_sequence
import random
from tqdm import tqdm

cfg = config()


def build_dictionary(dictionary_file_location):
    text_file = open(dictionary_file_location, "r")
    full_dictionary = text_file.read().splitlines()
    text_file.close()
    return full_dictionary


def word_encoder(word):
    word_encoded = []
    for i in word:
        if ord(i) == ord("_"):
            word_encoded.append(27)
        else:
            word_encoded.append(ord(i) - ord("a")+1)
    return word_encoded


def get_xy(full_dictionary):  # dataset preparation
    encoded_dictionary = []  # Encode the entire dictionary
    for i in full_dictionary:
        encoded_dictionary.append(word_encoder(i))

    X = []
    y = []
    for encoded_word in tqdm(encoded_dictionary):
        dic = {cha: [] for cha in list(set(encoded_word))}
        for i in range(len(encoded_word)):
            dic[encoded_word[i]].append(i)
        cha_lis = list(dic.keys())
        for cha, lis in dic.items():
            # We create a copy of the encoded word and mask (hide) every position of the occurence of the word
            masked_word = deepcopy(encoded_word)
            for pos in lis:
                masked_word[pos] = 27

            target_pos = cha - 1
            target = np.zeros(26)
            target[target_pos] = 1
            X.append(masked_word)
            y.append(target)
            times = 0
            seen = [cha]
            # A new masked word is generated by masking randomly more characters in the previously masked word
            new_masked_word = deepcopy(masked_word)
            while times < len(cha_lis):
                j = random.randint(0, len(cha_lis)-1)
                times += 1
                if cha_lis[j] in seen:
                    continue
                seen.append(cha_lis[j])
                for pos in dic[cha_lis[j]]:
                    new_masked_word[pos] = 27
                target[cha_lis[j]] = 1
                X.append(new_masked_word)
                y.append(target)

    len_max = 0
    for i in range(len(X)):  # Max length of the words is found and set
        X[i] = X[i][:-2]
        len_max = max(len_max, len(X[i]))

    X_tmp = []
    for i in tqdm(X):
        X_tmp.append(np.pad(i, (0, len_max-len(i)), 'constant', ))
    X = X_tmp
    # X = pad_sequences(X, maxlen=len_max, padding='post')

    # y = to_categorical(y, num_classes=26)
    eye = np.eye(26)
    # y = [eye[i] for i in y]
    train_x, test_x, train_y, test_y = train_test_split(
        X, y, test_size=cfg.test_size, random_state=cfg.seed)
    return (train_x, test_x, train_y, test_y)


def load_data(path):
    x_train = np.load(os.path.join(cfg.train_path, 'x_train.npy'))
    x_test = np.load(os.path.join(cfg.train_path, 'x_test.npy'))
    y_train = np.load(os.path.join(cfg.train_path, 'y_train.npy'))
    y_test = np.load(os.path.join(cfg.train_path, 'y_test.npy'))
    return x_train, x_test, y_train, y_test

class preprocess(Dataset):
    def __init__(self, x, y):
        self.x = np.array(x)
        self.y = np.array(y)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, i):
        x = torch.from_numpy(self.x[i])
        y = torch.from_numpy(self.y[i]).float()
        return x, y
